{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <H1> NAIVE BAYES CLASSIFIER </H1>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_wt</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>195</td>\n",
       "      <td>19</td>\n",
       "      <td>53</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>9</td>\n",
       "      <td>67</td>\n",
       "      <td>555</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>515</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name_wt  statuses_count  followers_count  friends_count  favourites_count  \\\n",
       "0  0.600000             195               19             53                58   \n",
       "1  0.705882               9               67            555                 2   \n",
       "2  0.916667              20               21            267                 0   \n",
       "3  0.500000              28               16            325                 0   \n",
       "4  0.733333              45               20            515                 0   \n",
       "\n",
       "   listed_count  label  \n",
       "0             0      0  \n",
       "1             1      0  \n",
       "2             0      1  \n",
       "3             0      1  \n",
       "4             0      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data/twitter_dataset.csv', encoding = 'latin-1')  #load data from csv\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2818, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name_wt',\n",
       " 'statuses_count',\n",
       " 'followers_count',\n",
       " 'friends_count',\n",
       " 'favourites_count',\n",
       " 'listed_count']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=[]\n",
    "for attributes in dataset.columns:\n",
    "    if attributes != 'label':\n",
    "        features.append(attributes)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 2D matrix \n",
    "data = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total instances :  2818 \n",
      "Number of features :  7\n"
     ]
    }
   ],
   "source": [
    "print(\"Total instances : \", data.shape[0], \"\\nNumber of features : \", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert label column into 1D arrray\n",
    "label = np.array(dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances:  2254\n",
      "Number of testing instances:  564\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    We have X_train, y_train, X_test, y_test.\n",
    "    Using these lists and dataframes we will randomly create two non-overlapping datasets \n",
    "        1. training set\n",
    "        2. testing set\n",
    "'''\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Number of training instances: \", X_train.shape[0])\n",
    "print(\"Number of testing instances: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "data = X_train\n",
    "label = y_train\n",
    "\n",
    "nb_model.fit(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model.predict([X_test[1]])    #testing for single instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   Now, apply the model to the entire test set and predict the label for each test example\n",
    "\n",
    "'''       \n",
    "       \n",
    "y_predict = []                       #to store prediction of each test example\n",
    "\n",
    "for test_case in range(len(X_test)): \n",
    "    label = nb_model.predict([X_test[test_case]])\n",
    "    \n",
    "    #append to the predictions list\n",
    "    y_predict.append(label.item())\n",
    "\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true negatives is C(0,0), false negatives is C(1,0), false positives is C(0,1) and true positives is C(1,1) \n",
    "conf_matrix = confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_negative\n",
    "TN = conf_matrix[0][0]\n",
    "#false_negative\n",
    "FN = conf_matrix[1][0]\n",
    "#false_positive\n",
    "FP = conf_matrix[0][1]\n",
    "#true_positive\n",
    "TP = conf_matrix[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall is the ratio of the total number of correctly classified positive examples divided by the total number of positive examples. \n",
    "# High Recall indicates the class is correctly recognized (small number of FN)\n",
    "\n",
    "recall = (TP)/(TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision is the the total number of correctly classified positive examples divided by the total number of predicted positive examples. \n",
    "# High Precision indicates an example labeled as positive is indeed positive (small number of FP)\n",
    "\n",
    "precision = (TP)/(TP + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875886524822695"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmeasure = (2*recall*precision)/(recall+precision)   #f-measure is the harmonice mean of Recall and Precision\n",
    "accuracy = (TP + TN)/(TN + FN + FP + TP) #Total number of correct predictions divided by total number of instances predicted\n",
    "\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ CLASSIFICATION PERFORMANCE OF THE NAIVE BAYES MODEL ------ \n",
      "\n",
      " Recall :  98.56630824372759 %\n",
      " Precision :  80.64516129032258 %\n",
      " Accuracy :  87.58865248226951 %\n",
      " F-measure :  88.70967741935483 %\n"
     ]
    }
   ],
   "source": [
    "print(\"------ CLASSIFICATION PERFORMANCE OF THE NAIVE BAYES MODEL ------ \\n\"\\\n",
    "      \"\\n Recall : \", (recall*100) ,\"%\" \\\n",
    "      \"\\n Precision : \", (precision*100) ,\"%\" \\\n",
    "      \"\\n Accuracy : \", (accuracy*100) ,\"%\" \\\n",
    "      \"\\n F-measure : \", (fmeasure*100) ,\"%\" )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
